\chapter{The Central Limit Theorem}
\label{ch:central-limit-theorem}

In this chapter the central limit theorem in introduced (cfr. Section~\ref{sec:central-limit-theorem}). This theorem is a key concept in probability theory, because multiple statistical testing procedures (cfr. Chapter~\ref{ch:testing-procedures}) are based on this. The central limit theorem allows you to generalize measurements obtained from a sample to the full population under certain conditions.

We also discuss the concept of point estimators and confidence intervals. A point estimator is a number that is calculated from the sample, which can be used to estimate a property of the population. For example, the mean/average of a sample $\overline{x}$ is a point estimator for the mean/average of the population $\mu$. A confidence interval is used to propose a range of plausible values for an unknown parameter together with an associated confidence level. For example, a 95\% confidence interval for the mean of a population is an interval derived from the sample, of which we can say that - under clear conditions - the population mean has a 95\% chance to be in the proposed range.

But before introducing these concepts, we first start with a short rehearsal of general concepts of the probability theory (cfr. Section~\ref{sec:probability-distribution-sample}) and discuss the normal distribution (cfr Section~\ref{sec:normal-distribution}).

\section{Learning Goals}
\label{sec:central-limit-theorem-learning-goals}

By the end of this chapter you must be able to:

\begin{itemize}
  \item Explain and apply the following concepts:
  \begin{itemize}
    \item Sample Space (universum), event, probability space, probability, discrete/continuous probability distribution
    \item Point Estimator, confidence interval
    \item Degrees of freedom
  \end{itemize}
  \item Sketch the probability distribution of the normal distribution for a given mean and standard deviation (Gaussian curve) and indicate the mean and standard deviation on this graph;
  \item Calculate the $z$ score of a value in a normally distributed random variable;
  \item Calculate the left- ($P(X<x)$) and right tail-probability ($P(X>x)$) or combinations of both (eg. $P(x<X<y)$) for a value in a normally distributed calculate stochastic variable, using the symmetry rule and the 100\% rule where necessary;
  \item Determine to what extent a given stochastic variable is normally distributed based on the probability density curve, QQ plot, skewness or kurtosis;
  \item Formulate the central limit theorem (and in particular know the probability distribution of the sample mean) and explain its importance for statistical analysis.
  \item Calculate a confidence interval for the population mean of a sample with a given confidence level in the following cases:
  \begin{itemize}
    \item a large sample with known population variance
    \item a small sample with unknown population variance
    \item a sampling fraction
  \end{itemize}
\end{itemize}

\section{Probability Distribution of a Sample}
\label{sec:probability-distribution-sample}

\subsection{Stochastic Experiment}
A random (or stochastic) experiment requires the following elements:

\begin{definition}[Sample Space or Universum]\
   The sample space or universum of an experiment
   is the collection of all possible outcomes of this experiment and
   is denoted by $\Omega$.
   \end{definition}

   \textbf{Remarks}
   \begin{itemize}
   \item The sample space needs to be \emph{complete} or \emph{collectively exhaustive}\/: 
   Every possible outcome of an experiment should be an element of $\Omega$.
   \item Additionally, it needs to be \emph{mutually exclusive}\/:
   Every outcome of an experiment should correspond with \emph{exactly one} element of $\Omega$.
   \item In summary: after executing an experiment it should be unambiguous 
   to indicate what element of $\Omega$ has occured.
   \end{itemize}
   
   \begin{definition}[Event]
    An \emph{event}\index{event} is a subset of the sample space. A singular or elementary event is a singleton. A composite event has a cardinality greater than 1.
\end{definition}

When $A$ and $B$ are events, the following events can be formed:
\begin{itemize}
    \item $A$ \textbf{or} $B$, noted as $A \cup B$;
    \item $A$ \textbf{and} $B$, noted as $A \cap B$;
    \item \textbf{not} $A$, noted as $\overline{A}$.
\end{itemize}

Events that don't have common outcomes are called \emph{disjoint}\index{event!disjoint}.
Consequently, disjoint events can never occur simultaneously. 
When events $A$ and $B$ are disjoint, then $A \cap B = \emptyset$. 

\textbf{Remarks}
\begin{itemize}
   \item It can be proven by induction that the union of $n$ events $A_1$ up to $A_n$ is also an event.
   \item The same goes for the intersection between events.
   \item For some application, countable infinite unions or intersections are also considered.
\end{itemize}
   
\begin{definition}[Probability space] 
    The probability of an event $A$ is noted as $P(A)$. Probabilities of events should meet the following requirements:

   \begin{enumerate}
   \item Probabilities of events are positive: $\forall A \subset \Omega: P(A) \geq 0$
   \item The total probability of all elements in $\Omega$ is 1: $P(\Omega) = 1.$
   \item If $A$ and $B$ are \emph{disjoint} events, then:
    \[P(A\cup B) = P(A) + P(B). \]
    This is called the Rule of Sum.
   \end{enumerate}

   When the probability function $P$ meets these requirements (axioms), 
   then the triple $(\Omega, \mathcal{P}(\Omega), P)$ is a \emph{probability space}\index{probability space} 
   (with $\mathcal{P}(\Omega)$ the \emph{power set} of $\Omega$, i.e.~the set of all its subsets).
\end{definition}
   
\begin{example}
\label{ex:dice}
    Consider a sample space $\Omega =  \left\{ 1,2,3,4,5,6 \right\} $ and a probability function $P(\omega) = \frac{1}{|\Omega|}$. 
    This combination could represent a dice with 6 sides, and outcomes $1, \ldots, 6$, each having a probability $P(\omega) = \frac{1}{6}$.
\end{example}
   
In this part of the syllabus we will look at \textbf{statistical inference}: make statements about the population based on a drawn sample.

\subsection{Probability distribution}
\label{ssec:probability-distribution}
   
\subsubsection{Discrete probability distribution}

If we continue the example of rolling a dice (cfr Example~\ref{ex:dice}), the probability of each outcome $\Omega = \{1,2,3,4,5,6\}$ can be summarized using a table or histogram (cfr. Figure~\ref{fig:probabilities-1-dice}). Some important notes:

\begin{enumerate}
    \item All probabilities are nonnegative.
    \item The probability of a specific outcome equals the area of the corresponding bar.
    \item The total area of all bars is 1.
\end{enumerate}

\begin{figure}
    \centering
    \begin{tikzpicture}
    \begin{axis}[ybar,ytick=data,ymin=0, ymax=.2, anchor=north, bar width=1, yscale=.5]
    \addplot[fill=cyan!20]
    coordinates {
        (1, 1/6)
        (2, 1/6)
        (3, 1/6)
        (4, 1/6)
        (5, 1/6)
        (6, 1/6)};
    \end{axis}
    \end{tikzpicture}
    \caption{Probability distribution when rolling a single dice.}
    \label{fig:probabilities-1-dice}
\end{figure}

When rolling \emph{two} dice, there are $6 \times 6 = 36$ possible outcomes, but some of them are equivalent. 
For example, $5$ and $2$, $2$ and $5$, $3$ and $4$, etc. all have a total outcome of $7$. 
There are two ways to roll a $3$. Consequently, $P(X=3) = \frac{2}{36}$. 
For $n = 1, \ldots, 7$, it holds that $P(X=n) = \frac{n-1}{36}$. 
Figure~\ref{fig:probabilities-2-dice} shows the corresponding histogram of the probability distribution.

\begin{figure}
    \centering
    \begin{tikzpicture}
    \begin{axis}[ybar,ytick=data, anchor=north, bar width=1, yscale=.5]
    \addplot[fill=cyan!20]
    coordinates {
      (2, 1/36)
      (3, 2/36)
      (4, 3/36)
      (5, 4/36)
      (6, 5/36)
      (7, 6/36)
      (8, 5/36)
      (9, 4/36)
      (10, 3/36)
      (11, 2/36)
      (12, 1/36)
    };
    
    \end{axis}
    \end{tikzpicture}
    \caption{Probability distribution when rolling two dice.}
    \label{fig:probabilities-2-dice}
  \end{figure}

The histogram allows for the following calculations:
\begin{itemize}
  \item The probability of rolling a 10 or higher is the sum of the area of bars 10, 11, and 12.
  \item The probability of rolling a number between 2 and 7 (not included) is the sum of the area of bars 3, \ldots, 6.
  \item $\ldots$
  \item The total area is 1: the probability that 1 of all these outcomes occurs is 100\%.
\end{itemize}

\subsubsection{Continuous probability distribution}

Continuous probability distributions\index{probability distribution!continuous} are distributions where the sample space doesn't merely consist of a limited number of outcomes (nominal or ordinal level of measurement), but where outcomes can be any numeric value (interval and ratio level).
Take for example the weight of our superheroes: this is a continuous variable since a value can be not only a whole number like $70$ kg or $95$ kg, but also (approximately) $86,8735485653$ kg. In principle, any real value is possible, although in practice it's impossible to measure this exactly. This has an important consequence for the probability distribution. The distribution no longer consist of separate bars, but has become a continuous curve (cfr. Figure~\ref{fig:verdelingReactievermogen}). This implies that the probability of measuring exactly $70$ kg is effectively zero. However, when we say $70$ kg, we usually mean any value between $69.5$ and $70.5$ kg, or more precisely the interval $[69.5, 70.5[$. Likewise, $70,00000$ kg could refer to a value within the interval $[69.999995, 70.000005[$ kg.

The properties of probability distributions mentioned before still hold. The surface below the curve totals to 1. Also, the probability between two values is the surface below the curve, between the boundaries defined by these values. Remark that it's not actually important whether the limits are included or not, since their probabilities are negligable.

\section{The normal distribution}
\label{sec:normal-distribution}

\begin{figure}[t]
\centering
\begin{tikzpicture}
\begin{axis}[
  domain=0:10, samples=100,
  axis lines*=left, xlabel=$x$, ylabel=$y$,
  every axis y label/.style={at=(current axis.above origin),anchor=south},
  every axis x label/.style={at=(current axis.right of origin),anchor=west},
  height=5cm, width=12cm,
  xtick={5,3.5,6.5}, ytick=\empty,
  enlargelimits=false, clip=false, axis on top,
  grid = major
  ]
  \addplot [fill=cyan!20, draw=none, domain=0:9] {gauss(5,1.5)} \closedcycle;
  \draw [yshift=-0.6cm, latex-latex](axis cs:3.5,0) -- node [fill=white] {$\sigma$} (axis cs:5.0,0);
\end{axis}
\end{tikzpicture}
\caption{The probability of Superman's reaction speed. This curve illustrates the normal distribution with mean $\mu = 5$ ms and standard deviation $\sigma = 1.5 ms.$}
\label{fig:verdelingReactievermogen}
\end{figure}


Figure \ref{fig:verdelingReactievermogen} illustrates the probability distribution of Superman's reaction speed, variable $X$. The curve follows the normal distribution\index{distribution!normal} with a mean of 5 ms and a standard deviation of 1.5 ms. This is also notated as:
\[ X  \sim Nor(\mu = 5; \sigma = 1.5) \]

The equation of this function, the probability density, is sometimes referred to as the Gaussian bell curve, called after mathematician Carl Friedrich Gauss:
\begin{equation}
  f(x) = \frac{1}{\sigma \sqrt{2\pi}} e^{-\frac{1}{2} \frac{(x - \mu)^{2}}{\sigma^{2}}}
  \label{eq:normalFunction}
\end{equation}

De normal distribution has the following properties:
\begin{itemize}
  \item The probability density is bell-shaped;
  \item The probability density is symmetric around $\mu$;
  \item Because of this symmetry, the mean, median and mode are equal;
  \item The total area below the bell curve and above the $x$-axis is 1;
  \item The area between $\mu - \sigma$ en $\mu + \sigma$ (the so-called sigma area) contains approximately 68\% of the observations;
  \item The area between $\mu - 2 \sigma$ and $\mu + 2 \sigma$ contains about 95\% of all observations;
  \item The area between $\mu - 3 \sigma$ and $\mu + 3 \sigma$ contains about 99.7\% of all observations;
  \item The different areas are illustrated in Figure~\ref{fig:standard-normal-distribution}.
\end{itemize}

\subsection{The standard normal distribution}
\label{ssec:standard-normal-distribution}

If the distribution of stochastic variable $X$ is $X \sim N(\mu, \sigma)$, then the distribution of $Z = \frac{X - \mu}{\sigma}$ is $Z \sim N(0,1)$. This is called the standard normal distribution\index{distribution!standard normal}.

% Bron: http://johncanning.net/wp/?p=1202
\begin{center}
\begin{figure}
\centering
\begin{tikzpicture}
    \begin{axis}[
        no markers, domain=0:10, samples=100,
        axis lines*=left,height=6cm, width=10cm,
        xtick={-3, -2, -1, 0, 1, 2, 3}, ytick=\empty,
        enlargelimits=false, clip=false, axis on top,
        grid = major
    ]
    \addplot [smooth,fill=cyan!20, draw=none, domain=-3:3] {gauss(0,1)} \closedcycle;
    \addplot [smooth,fill=orange!20, draw=none, domain=-3:-2] {gauss(0,1)} \closedcycle;
    \addplot [smooth,fill=orange!20, draw=none, domain=2:3] {gauss(0,1)} \closedcycle;
    \addplot [smooth,fill=blue!20, draw=none, domain=-2:-1] {gauss(0,1)} \closedcycle;
    \addplot [smooth,fill=blue!20, draw=none, domain=1:2] {gauss(0,1)} \closedcycle;
    \addplot[<->] coordinates {(-1,0.4) (1,0.4)};
    \addplot[<->] coordinates {(-2,0.3) (2,0.3)};
    \addplot[<->] coordinates {(-3,0.2) (3,0.2)};
    \node[coordinate, pin={68.3\%}] at (axis cs: 0, 0.35){};
    \node[coordinate, pin={95.4\%}] at (axis cs: 0, 0.25){};
    \node[coordinate, pin={99.7\%}] at (axis cs: 0, 0.15){};
    \node[coordinate, pin={34.1\%}] at (axis cs: -0.5, 0){};
    \node[coordinate, pin={34.1\%}] at (axis cs: 0.5, 0){};
    \node[coordinate, pin={13.6\%}] at (axis cs: 1.5, 0){};
    \node[coordinate, pin={13.6\%}] at (axis cs: -1.5, 0){};
    \node[coordinate, pin={2.1\%}] at (axis cs: 2.5, 0){};
    \node[coordinate, pin={2.1\%}] at (axis cs: -2.5, 0){};
    \end{axis}
\end{tikzpicture}
\caption{The standard normal distribution with different ``zones'' indicating the percentage of observations within each zone.}
\label{fig:standard-normal-distribution}
\end{figure}
\end{center}

Often, it is useful to compare observations from different normal distributions. We can \emph{normalize} an observation by calculating the distance from the mean in terms of the standard deviation. This is called the $z$-score and is calculated as follows:

\begin{equation}
    z = \frac{x-\mu}{\sigma}
    \label{eq:zscore}
  \end{equation}

This score indicates how extreme an observation is, or in other words how many standard deviations it is located away from the mean. 
For a random value of $x$ we can use Equation~\ref{eq:zscore} to calculate the corresponding $z$-score.
The $z$-scores are so significant and often used that specific tables were composed containing the probabilities that an observation drawn from $Z$ is smaller than $z$, the so-called left tail probability\footnote{Similar tables containing the right tail probability exist as well}: $P(Z<z)$.

R also has functions for calculating with probabilities of normally distributed variables. These are summarized in Table~\ref{rab:norm-prob-r}.

\begin{table}
  \centering
  \begin{tabular}{ll}
  	\textbf{Function}     & \textbf{Description}                                           \\ \hline
  	\verb|pnorm(x, m, s)| & Left tail probability, $P(X<\mathtt{x})$                       \\
  	\verb|dnorm(x, m, s)| & Altitude of the Gaussian curve at point \texttt{x}             \\
  	\verb|qnorm(p, m, s)| & What boundary value contains \texttt{p}\% of the observations?\\
  	\verb|rnorm(n, m, s)| & Generate \texttt{n} normally distributed random numbers
  \end{tabular}

  \caption{Functions for calculating the probability in R for a normal distribution with mean \texttt{m} and standard deviation \texttt{s}. If the values for arguments \texttt{m} and \texttt{s} are omitted, the standard normal distribution is used.}
  \label{rab:norm-prob-r}
\end{table}

In order to calculate probabilities for any normal distribution, the following method can be used:

\begin{enumerate}
  \item Determine the stochastic variable with the associated normal distribution ($\mu$ and $\sigma$);
  \item Calculate the $z$-score for a given $x$-value;
  \item Reduce the requested probability to a left tail probability, using the probabilities of the normal distribution:
  \begin{itemize}
    \item $P(Z > z) = P(Z < -z)$ (symmetry rule);
    \item $P(Z > z) = 1 - P(Z < z)$ (100\% probability rule)
  \end{itemize}
\end{enumerate}

Plotting or sketching the requested probability is also very useful to gain insight into the calculation.

\begin{example}
What is the probability that Superman reacts in less than 4 ms?
\[ P(X < 4) = P(Z < -0,67) = 0,2514 \]
\end{example}

\begin{example}
What is the probability that he reacts in less than 7 ms?
\[ P(X < 7) = P(Z < 1,33) = 0,9082 \]
\end{example}

\begin{example}
What is the probability that he reacts in less than 3 ms?
\[ P(X<3) = P(Z < -1,33) = 0,0918 \]
\end{example}

\begin{example}
What is the probability that he reacts between 2 en 6.5 ms?
\[ P( 2 < X < 6,5) = P(X<6,5) - P(X<2) = P(Z<1) - P(Z<-2) = 0,8186 \]
\end{example}

\subsection{Testing for normality}
\label{ssec:testing-for-normality}

Because of the desirable properties of the normal distribution, it's often good to know whether a sample is actually drawn from a normal distribution. Several methods exist to test for normality:

\begin{enumerate}
  \item Plot a histogram for the data and look at the shape of the chart. If the observations are drawn from a normal distribution, the shape will approximate a bell curve.
  \item Calculate the intervals $\overline{x} \pm s$, $\overline{x} \pm 2s$, $\overline{x} \pm 3s$ and determine the percentage of observations in each interval. If the observations are normally distributed, these percentages should be around 68\%, 95\%, and 99,7\%, respectively.
  \item Draw a Q-Q plot\index{Q-Q plot} (normality plot, cfr. Definition~\ref{def:qq-plot}) for the observations. If they are normally distributed, the observations will be plotted approximately on a straight line.
  \item Calculate the \emph{kurtosis}\index{kurtosis}, a metric for the ``sharpness'' of the distribution's ``peak'':
    \begin{itemize}
      \item A normal distribution has a kurtosis of 3, or, alternatively, an \emph{excess kurtosis}\index{kurtosis!excess} of 0;
      \item A ``flat'' distribution has a negative excess kurtosis;
      \item A distribution with a sharp peak has a positive excess kurtosis.
      \item Note: when using the original definition of kurtosis, the normal distribution has a kurtosis of 3. We however use an alternative definition of kurtotis here, often referred to as the ``excess kurtosis'', which substracts 3 of the original value, resulting in a kurtosis of 0 for a normal distribution.
    \end{itemize}
  \item Calculate the \emph{skewness}\index{skewness}, a metric for the symmetry of the data:
    \begin{itemize}
      \item A symmetric distribution (including the normal distribution) has a skewness of 0;
      \item A distribution with a long left tail has a negative skewness;
      \item A distribution with a long right tail has a positive skewness;
      \item Rule of thumb: if the absolute value of the skewness $>1$, the distribution is not considered to be symmetrical.
    \end{itemize}
\end{enumerate}

\begin{definition}[Q-Q plot or normality plot]
    \label{def:qq-plot}
    A normality plot, or \emph{Q-Q plot}\index{Q-Q plot}\footnote{Q stands for quantile} for a set of observations is a probability plot with the sorted observations on one axis and the associated expected $z$-values on the other axis. See Figure~\ref{fig:qqplot} for some examples. The code in R used for generating the images is given below. If the data is normally distributed, the observations will be plotted on or near the line $y = x$.
  \end{definition}

\begin{figure}
  \begin{center}
    \includegraphics[width=.45\textwidth]{sampling-qqplot-good}
    \includegraphics[width=.45\textwidth]{sampling-qqplot-bad}
  \end{center}
  \caption{The Q-Q plot on the left is based on a sample of 50 observations drawn from a normal distribution with mean 1000 and standard deviation 50. The plot on the right is based on a sample of 50 observations drawn from a Student's $t$-distribution with 15 degrees of freedom, with the same mean and standard deviation. The lines in red indicate where the observations should theoretically be situated. In the plot on the left, this is more or less the case, but on the right, especially in the extremes, observations deviate from the line.}
  \label{fig:qqplot}
\end{figure}

\lstinputlisting{data/qqplot.R}

\section{The Central Limit Theorem}
\label{sec:central-limit-theorem}

In this section, we'll discuss one of the most fundamental results in statistics and the foundation of sampling: the central limit theorem.

\begin{definition}[Linear combination of independent, identically distributed stochastic variables]
  Formal: A linear combination of a sufficiently large number of independent, identically distributed stochastic variables (i.e. with a well defined expected value and variance) will approximate the normal distribution, regardless of the underlying distribution.

  \[X_{i} \sim Nor(\mu_{i}, \sigma_{i}) \Rightarrow Y = \sum_{i} \alpha_{i} X_{i} \textnormal{ approximates the normal distribution} \]

  Consequently, the mean of a sample derived from a population with a random distribution will approximate a normal distribution for a sufficiently large value of $n$.
\end{definition}

Therefore, given a random sample of independent variables with a normal distribution,
the central limit theorem states that the mean of this sample will also approximate the normal distribution.
So, if you would repeatedly take a sample of the same size, and measure the mean, the resulting plot will approximate the graph of a normal distribution (cfr. Figure~\ref{sec:normal-distribution}).
The larger the sample, the closer the approximation will be.
The mean of the sample therefore has a normal distribution, independent of the underlying distribution of the metric from which a sample is taken. 

In general:

\begin{definition}[The Central Limit Theorem]
  Consider a random sample of $n$ observations drawn from a population with expected value $\mu$ and standard deviation $\sigma$.
  If $n$ is sufficiently large, the probability distribution of the sample mean $\overline{x}$ will approximate a normal distribution with expected value $\mu_{\overline{x}} = \mu$ and standard deviation $\sigma_{\overline{x}} = \frac{\sigma}{\sqrt{n}}$.
  The larger the sample, the better the probability distribution of $\overline{x}$ will approximate the expected value of the population, $\mu$.
\end{definition}

When taking a sample, the underlying distribution is rarely known, and yet one can make statements about the sample mean. This is entirely thanks to the central limit theorem, which imposes a rule about the mean regardless of the underlying probability distribution. 
The central limit theorem keeps the sample mean under control, locking it in the Gaussian cage from which it can never escape. This, and only this, allows scientists to study it carefully, observe it, and enables them to formulate conclusions.

Because, if the distribution of the sample mean would depend on the underlying distribution, which would be expected to some extent, it would be impossible to make concrete statements about many scientific results. 
In statistics theory, limits of sample means appear everywhere, and they can be easily replaced by a normal distribution thanks to the central limit theorem. 
If this would not be possible, the whole theory of estimating parameters would collapse, which would be disastrous in practice. 
Comparing research would be reduced to an almost impossible task, and statistics in general would become much more difficult and complicated.

The proof of the central limit theorem is outside the scope of this course, even though it is surprisingly easy to understand.

\subsection{Application of the Central Limit Theorem}
When drawing a random sample of a sufficiently large size $n$ form a population with an unknown $\mu$ and (known) standard deviation $\sigma$, the probability distribution of the sample mean is a stochastic variable $M \sim N (\overline{x}, \frac{\sigma}{\sqrt{n}})$.

\begin{example}
  Let's consider the reaction times of all our superheroes and assume we have drawn a random sample with $n = 100$, $\overline{x} = 90$, and $\sigma = 60$ (ms).
  We can then ask ourselves: what is the probability that the average reaction time of a superhero is below $104$ ms?

  \begin{enumerate}
    \item In this example, the stochastic variable is the average reaction speed $\overline{x}$ in a sample of $n=100$ superheroes. Because of the central limit theory, the following holds:
    \[ \overline{x} \sim Nor(\mu = 90, \sigma_{\overline{x}} = \frac{60}{\sqrt{100}} = 6) \]
    \item We can calculate the associated $z$-score:
    \[ z = \frac{104-90}{\frac{60}{\sqrt{100}}} = \frac{104-90}{6} = 2.33 \]
    \item And therefore: $P(\overline{x} < 104) = P(Z < 2.33) = 1 - 0.0099 \approx 0.99$ or about 99\%
  \end{enumerate}
\end{example}

\subsection{Estimating Stochastic Parameters}
\label{ssec:estimating-stochastic-parameters}

If we want to study a sample, it is usually to draw conclusions about the population as a whole. 
For example, we want to know the average strength of a superhero, or the fraction of rich superheroes. 
In general, we'll \emph{estimate} a parameter of the population based on the sample. For example, $\overline{x}$ is used as an estimate of $\mu$. This type of estimation is defined as:

\begin{definition}[point estimate]
  A \emph{Point Estimate}\index{estimate!point} for a population parameter is a formula or equation that allows us to calculate a value to estimate that parameter.
\end{definition}

Another type of estimates are \emph{interval estimates}\index{estimate!interval}, a.o.~confidence intervals. These are discussed in the next sections.

\subsection{Confidence interval for the population mean of a large sample}
\label{ssec:confidence-interval-pop-mean-large-sample}

When we estimate the population mean from a sample, we have no idea how correct this estimate is. However, the properties of the normal distribution allow us to construct an interval that will contain the population mean with the desired level of confidence.

\begin{definition}[Confidence Interval]
  A \emph{Confidence Interval}\index{confidence interval} is an equation or formula that allows us to construct an interval that will contain the parameter to be estimated with a certain level of confidence.
\end{definition}

An initial estimate for the population mean is of course the sample mean:

\[ \overline{x} = \frac{1}{n} \sum_{i} x_{i} \]

Of course this estimate is not the real population mean. However, because of the central limit theorem, we know that the mean of a sample of size $n$ is normally distributed with mean $\mu$ and standard deviation $\frac{\sigma}{\sqrt{n}}$.
After standardisation, we get:
\[ Z = \frac{\overline{x} - \mu}{\frac{\sigma}{\sqrt{n}}} \]

This expression depends on $\mu$, but we know that this parameter has a normal distribution. As a result, we can find limits $-z$ and $z$, independent from $\mu$, that will contain $Z$ with a \emph{confidence level} $1 - \alpha$ (which can be chosen by the researcher). In this example, we choose $1 - \alpha= 0.95$ (and consequently $\alpha = 0.05$).

\[P(-z < Z < z) = 1 - \alpha = 0.95 \]

After applying the rule of symmetry, we find that we need to determine $z$ by:

\[ P( Z < z) = 1 - 0,025 = 0.975 \]

When we look this up in a Z-table, we'll find that $z = 1.96$. Or, alternatively, we can calculate \verb|qnorm(0.975)| in R.

The confidence interval then is:

\[ P( -1.96 < \frac{\overline{x} - \mu}{\frac{\sigma}{\sqrt{n}}} < 1.96 ) = 1 - \alpha\]

and therefore
\[ P ( \overline{x} -1.96 \frac{\sigma}{\sqrt{n}} <\mu < \overline{x} + 1.96 \frac{\sigma}{\sqrt{n}}) = 1 - \alpha \]

Using this technique, we can determine the bounds of an interval that will contain $\mu$ with a confidence level of 95\%. 
If you would repeatedly draw a sample from this population and calculate the confidence interval around $\overline{x}$, then in 95\% of the cases, we expect the actual $\mu$ to be inside the interval bounds.

However, note that we assume to know the standard deviation of the population, which will generally not be the case. If the sample size is sufficiently large, the standard deviation of the sample is used as a point estimate for the standard deviation of the population.

\[ P ( \overline{x} -1,96 \frac{\sigma_{\overline{x}}}{\sqrt{n}} < \mu < \overline{x} + 1,96 \frac{\sigma_{\overline{x}}}{\sqrt{n}}) = 1 - \alpha \]


\begin{figure}
\centering
\begin{tikzpicture}
\begin{axis}[
  domain=-3:3, samples=100,
  axis lines*=left, xlabel=$z$,
  every axis y label/.style={at=(current axis.above origin),anchor=south},
  every axis x label/.style={at=(current axis.right of origin),anchor=west},
  height=5cm, width=12cm,
  xtick={-1.96,0,1.96}, ytick=\empty,
  enlargelimits=false, clip=false, axis on top,
  grid = major
  ]
  \addplot [fill=cyan!20, draw=none, domain=-3:3] {gauss(0,1)} \closedcycle;
  \draw [yshift=-0.6cm, latex-latex](axis cs:-1.96,0) -- node [fill=white] {$\sigma$} (axis cs:1.96,0);
\end{axis}
\end{tikzpicture}
\caption{Standard normal distribution with a 95\% confidence interval.}
\label{fig:verdelingStandaardnormaal}
\end{figure}

\subsection{Confidence interval for the population mean of a small sample}
\label{ssec:confidence-interval-pop-mean-small-sample}

In the case of a small sample, we can no longer assume that the probability distribution of $\overline{x}$ approximates the normal distribution. 
The central limit theorem only holds for large sample sizes of about $n > 30$.

The shape of the probability distribution of $\overline{x}$ now depends of the shape of the underlying probability distribution of the population. 
Although $\sigma_{\overline{x}} = \frac{\sigma}{\sqrt{n}}$ still holds, the standard deviation of the sample $s$ 
could be a bad estimate for $\sigma$ if the sample is small.

However, there is a solution in the form of the so-called Student's $t$-distribution. Instead of calculating the z-score:
\[ z = \frac{\overline{x} - \mu}{\frac{\sigma}{\sqrt{n}}} \]

we can use the $t$-score:
\[ t = \frac{\overline{x} - \mu}{\frac{s}{\sqrt{n}}} \]

The probability density of the Student's $t$-distribution is very similar to the standard normal distribution: 
bell-shaped, symmetrical, and with an expected value of 0.

The exact shape depends on the sample size $n$, or more precisely on the degrees of freedom\index{degrees of freedom} $(n-1)$ (abbreviated as $df$).

Note that:
\begin{itemize}
  \item $(n-1)$ is also used to calculate the sample variance $s^{2}$;
  \item When $n \rightarrow \infty$ this converges to the standard normal distribution.
\end{itemize}

To calculate a confidence interval for a sample with a small number of observations, we need to do the following:

\begin{definition}[Confidence Interval for a Small Sample]
  To determine a confidence interval for the average based on a small sample, we calculate:
  \[ \overline{x} \pm t_{\frac{\alpha}{2}}(\frac{s}{\sqrt{n}}) \]
  in which $t_{\frac{\alpha}{2}}$ is based on $(n-1)$ degrees of freedom. 
  We still assume that the sample is random and drawn from a population that is approximately normally distributed.
\end{definition}

\begin{table}
  \centering
  \begin{tabular}{ll}
    \textbf{Function}     & \textbf{Description}                                      \\
    \midrule
  	\verb|pt(x, df)| & Left tail probability, $P(X<\mathtt{x})$                       \\
  	\verb|dt(x, df)| & Altitude of the Gaussian curve at point \texttt{x}             \\
  	\verb|qt(p, df)| & What boundary value contains \texttt{p}\% of the observations? \\
  	\verb|rt(n, df)| & Generate \texttt{n} random numbers using the $t$ distribution  \\ 
  \end{tabular}

  \caption{Functions for calculating the probability in R for the Student's $t$-distribution with \texttt{df} degrees of freedom, expected value 0 and standard deviation 1.}
  \label{tab:t-prob-r}
\end{table}

\subsection{Confidence interval for a population fraction of a large sample}
\label{ssec:confidence-interval-pop-fraction-large-sample}

If you want to measure a variable as a fraction, for example the percentage of people that responded with ``yes'' to a specific question, this is equivalent to estimating the probability $p$ on success in a binomial experiment, i.e.~an experiment with a sample space consisting of two elements, ``success'' and ``failure''. 
If the probability of ``success'' is $p$, the probability of ``failure'' is $q = 1 - p$. 
The value of $p$ can be estimated based on the sample:

\[ \overline{p} = \frac{\textnormal{number of successes}}{n} \]

In order to calculate a confidence interval for $\overline{p}$, we need to know the probability distribution of $\overline{p}$. 
The central limit theory can then be applied to the average number of successes in a sample of size $n$. 
If success = 1 and failure = 0, we have a sample of $n$ elements, each having the same distribution (probability of 1 is $p$ and probability of 0 is $q=1-p$).
The probability distribution of $\overline{p}$ has the following properties:

\begin{itemize}
  \item The expected value of the probability distribution of $\overline{p}$ is $p$.
  \item The standard deviation of the probability distribution of $\overline{p}$ is $\sqrt{\frac{pq}{n}}$.
  \item For large samples, $\overline{p}$ is approximately normally distributed.
\end{itemize}

Since $\overline{p}$ is the sample mean of the number of successes, this allows us to calculate a confidence interval, analougous to the one for $\mu$ for large samples.

\begin{definition}[Confidence interval for $p$ for a large sample]
  \[ \overline{p} \pm z_{\frac{\alpha}{2}} \sqrt{\frac{\overline{p}\overline{q}}{n}} \]
  with $\overline{p} = \frac{x}{n}$ and $\overline{q} = 1- \overline{p}$
\end{definition}


\section{R}
We take a look at some basic operations related to several probability distributions.
Although R knows many different distributions, we will only focus on the main ones.
To find out all supported distributions, you can use the following command in R:

\begin{lstlisting}
> help.search ("distribution")
\end{lstlisting}

In this section we will provide some details about the command used for a normal distribution, and briefly mention the command for other distributions.
The functions for the different distributions are very similar.

The prefixes are as follows:
\begin{description}
	\item[d] returns the altitude of the selected curve
	\item[p] returns the cumulative probability density function
	\item[q] returns the reverse cumulative density function
	\item[r] returns a random value
\end{description}

\subsection{The Normal Distribution}
There are four functions to generate values associated with the normal distribution.
\subsubsection{dnorm}

When a value is provided, this function returns the height of the probability distribution for each requested point.
If no other parameters are provided, without mean or standard deviation, the standard normal distribution is used (with mean 0 and standard deviation 1).
However, it is also possible to use a different value for the average and standard deviation by adding the corresponding parameters.

\lstinputlisting{data/norm.R}

\subsubsection{pnorm}

This function returns the cumulative probability density function, or in other words the left tail probability: \texttt{pnorm(x)} is $P(Z < x)$.

\subsubsection{qnorm}
The function \texttt{qnorm} is the inverse of \texttt{pnorm}.
The main idea behind \texttt{qnorm} is that when you provide a probability $\alpha$, 
this function returns the value for which the left tail probability is $\alpha$.

\lstinputlisting{data/qnorm.R}

\subsubsection{rnorm}

The \texttt{rnorm} function can generate random values using a normal distribution.
The first argument is the required number of values, and a custom average and standard deviation can also be supplied as optional parameters.

\lstinputlisting{data/rnorm.R}




% Exercises 
\section{Exercises}
\label{sec:sampling-exercises}

\begin{exercise}
  \label{ex:prob-norm-dist}
  Calculate the following probabilities, either using a $z$ table, or using the appropriate R function. Also draw a sketch of the area.
  \begin{enumerate}[label=\alph*.]
    \item $P(Z < 1.33)$
    \item $P(Z > 1.33)$
    \item $P(Z < -1.33)$
    \item $P(Z > -1.33)$
    \item $P(Z < 0.45)$
    \item $P(Z > -1.05)$
    \item $P(Z < 0.65)$
    \item $P(-0.45 < Z < 1.20)$
    \item $P(-1.35 < Z < -0.10)$
    \item $P(-2.10 < Z < -0.90)$
  \end{enumerate}
\end{exercise}

\begin{exercise}
  Determine the probability distribution and the cumulative probability curve for a normal distribution with mean $\mu = 2.5$ and standard deviation $\sigma = 1.5$.
  Determine the area of the surface below the density curve between $ x = 0.5 $ and $ x = 4 $.
  Verify your answer by doing the calculation.
\end{exercise}

\begin{exercise}
  Determine the probability distribution and cumulative probability curve for a $t$-distribution with $df = 3$.
  Also draw a normal distribution with $\mu = 0$ and $\sigma = 1$.
\end{exercise}

\begin{exercise}
  Use the \verb|rnorm()| function to generate a random sample of 25 values based on a normal distribution with mean $\mu = 0$ and standard deviation $\sigma = 1$.
  Draw a histogram, using \verb|probability = TRUE|.

  Draw an overlay on the histogram consisting of (a) the theoretical probability distribution curve of a normal distribution with mean $\mu = 0$ and standard deviation $\sigma = 1$;
  and (b) an ``estimated'' probability distribution curve based on the measured sample mean and standard deviation.

  Repeat this for a sample consisting of 100 and 500 values.
\end{exercise}

\begin{exercise}

  The course of research techniques at a college is organised in two separate classes. 
  Students were distributed at random over the two classes, A and B, so we can assume that one class is not significantly better than the other. 
  Class A is lectured by Mrs. X, class B by Mr. Y.

  Mrs. X is rather strict and her group gets grades with an average of 54/100 and a standard deviation of 11.

  Y is less strict and is known for stimulating his students by giving them a bonus point at times. By the end of the academic year, the grades for his group have an average of 62/100 with a standard deviation of 7.

  Wouter is a student of class A and got a grade of $\frac{63}{100}$, Stijn from class B got $\frac{67}{100}$. Who has the best result?
\end{exercise}

\begin{exercise}
  A survey conducted between 1988 and 1994 indicated that the average cholesterol level of women between 20 and 29 years was 183 mg/dl, with a standard deviation of 36. 
  The sample consisted of 81 women, and we assume that the sample was taken at random.

  \begin{enumerate}[label=\alph*.]
    \item Plot the probability distribution for the sample mean $\overline{x}$.
    \item What is the probability that $\overline{x}$ is smaller than 185?
    \item What is the probability that $\overline{x}$ is between 175 and 185?
    \item What is the probability that $\overline{x}$ is larger than 190? 
  \end{enumerate}
\end{exercise}

\begin{exercise}
  A random sample consisting of 64 observations is drawn from a population with an unknown distribution. 
  However, the population mean and standard deviation are known: $\mu = 20$ and $\sigma = 16$.

  \begin{enumerate}[label=\alph*.]
    \item What are the mean and standard deviation of the sample?
    \item Describe the shape of the probability distribution of the sample mean. Does this depend on the sample size?
    \item Calculate the $z$-score for $\overline{x_{1}} = 15.5$ and $\overline{x_{2}} = 23$.
    \item Calculate the probability that $\overline{x} < 16$.
    \item Calculate the probability that $\overline{x} > 23$.
    \item Calculate the probability that $16 < \overline{x} < 22$.
  \end{enumerate}
\end{exercise}

\begin{exercise}
  Speed bumps are designed to influence the speed of drivers. 
  Depending on the desired speed in a street, speed bumps can be made steeper or more gradual.
  Speed bump A is designed so that 85\% of drivers pass over with a maximum speed of 50 km/h. 
  The speed of the drivers appears to be normally distributed. A sample indicates that the average speed is 43.1 km/h with a standard deviation of 6.6 km/h.

  \begin{enumerate}[label=\alph*.]
    \item Prove that 85\% of drivers actually do not drive faster than 50 km/h.
    \item If the sample size was 1200, what is the expected probability of a driver passing with a speed of at least 55 km/h?
  \end{enumerate}
\end{exercise}

\begin{exercise}
  Lately, a canned goods manufacturer received complaints about the net contents of their carrot and peas cans. 
  According to the label, the can should contain 1 liter. 
  To verify these complaints, the QA department draws a random sample of 40 cans and checks the contents. 
  The results are provided in Table~\ref{tab:canned-goods-sample-values}.

  A. Add the following data to the table:
  \begin{itemize}
    \item the cumulative absolute frequency
    \item the relative frequency
    \item the cumulative relative frequency
  \end{itemize}

  B. Calculate:
  \begin{itemize}
    \item The mean and standard deviation of the sample
    \item The percentage of cans for which the contents are too low
    \item Plot a histogram of the absolute frequency
    \item Are the observations normally distributed? How can you determine this?
  \end{itemize}
\end{exercise}

\begin{table}
  \centering
  \begin{tabular}{lr}
    \toprule
    Contents & $n_{i}$ \\
    \midrule
    $[970,980[$ & 3 \\
    $[980,990[$ & 5 \\
    $[990,1000[$ & 13 \\
    $[1000,1010[$ & 11 \\
    $[1010,1020[$ & 5 \\
    $[1020,1030[$ & 3 \\
    \bottomrule
  \end{tabular}
  \caption{Sample Values}
  \label{tab:canned-goods-sample-values}
\end{table}

\begin{exercise}
  Confidence intervals.
  
  \begin{enumerate}
    \item What are the lower and upper bounds for a confidence interval of 99\%?
    \item A confidence interval of 99\% is wider than a confidence interval for 95\%. Why?
    \item What would be the confidence interval for a confidence level of 100\%?
  \end{enumerate}
  
\end{exercise}

\begin{exercise}
  \label{ex:confidence-money}
  
  Import the dataset \texttt{money.csv} in R. 
  We assume that the values of this sample are normally distributed with 
  an unknown population mean $\mu$, but the standard deviation of the population
  is supposed to be known; $\sigma = 99$. 
  
  \begin{enumerate}
    \item Determine a 99\% confidence interval for the population mean.
    \item Determine a 95\% confidence interval for the population mean.
    \item Suppose that $\sigma$ is also unkown. In this case, determine a 95\% confidence interval for the population mean.
    \item Determine a 95\% confidence interval for the population mean, assuming that the sample consists of only the 25 first values of the imported file.
  \end{enumerate}
\end{exercise}

\begin{exercise}
  A web hosting company has a Service Level Agreement with a customer that guarantees an uptime of ``five nines'' (99.999\%). 
  This is verified at the end of each year and if the minimal uptime is not met, a fine is imposed.

  To measure the uptime, a monitoring system queries the web server every minute using a \texttt{HTTP GET /} request and verifies the result using the HTTP return code. 
  In january, a single HTTP request was unsuccesful.

  \begin{itemize}
    \item If the current trend continues, what is the probability that the SLA is not met at the end of the year? Use the formula for the probability distribution of a fraction.
    \item In this particular case, the formula actually isn't suitable, and gives a slightly distorted result. Why?
  \end{itemize}
\end{exercise}






% Answers
\section{Solutions to selected exercises}
\label{sec:oplossingen-steekproefonderzoek}

\paragraph{Exercise \ref{ex:prob-norm-dist}}

\begin{enumerate}[label=\alph*.]
  \item $0.908$
  \item $0.092$
  \item $0.092$
  \item $0.908$
  \item $0.674$
  \item $0.853$
  \item $0.742$
  \item $0.559$
  \item $0.372$
  \item $0.166$
\end{enumerate}

\paragraph{Exercise \ref{ex:confidence-money}}

\begin{enumerate}
  \item $[481.725; 523.368]$
  \item $[486.704; 518.390]$
  \item $[486.476; 518.617]$
  \item $[485.761; 519.332]$
\end{enumerate}

   