---
title: "6 -- Bivariate analysis"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

We are using the result of a survey amongst the customers of a restaurant of a university college. Don't forget to set the working directory with "Session > Set Working Directory > To Source File Location", otherwise the data file won't be found.

```{r}
library(foreign)
resto <- read.spss("../syllabus/data/catering_college.sav",
                   to.data.frame = TRUE)
```

## Chi-square and Cramér's V

Chi-square and Cramér's V are measures to check if there is a relation between 2 qualitative variables (in R: "factors").

Let's check if there is a relation between the sex (Geslacht) and the appreciation of the base menu (Keuze_basis).
If the answers from men and women are (more or less) similar, then we will say that there is *no* relation between sex and appreciation.

```{r}
# Calculate frequency table.
# First the dependent variable, second the independent variable
observed <- table(resto$Keuze_basis,
                  resto$Geslacht)
# Add maginal totals
addmargins(observed)
```
(vrouw=woman, man=man, Goed=good, Voldoende=sufficient, Onvoldoende=insufficient, Slecht=bad)

### Calculations

First we calculate the values we would expect if there was no relation between sex and appreciation.
These values will be similar for men and women.
You can calculate these values by multiplying the row total with the column total and dividing by the total amount of answers.
For the first cel, for example, you get `22 * 17 / 49`.

```{r}
row_sums <- rowSums(observed)              # rijtotalen
col_sums <- colSums(observed)              # kolomtotalen
n <- sum(observed)                         # total entire table (= number of answers)
expected <- outer(row_sums, col_sums) / n  # expected values
addmargins(expected)                       # add margin totals
```

The margin totals are exactly the same as in the first table.
The frequencies for every cell are slightly deviating.

A good measure for this deviation is to take the difference between the expected and the observed values, to square (to get positive values) and to divide by the expected values:

```{r}
diffs <- (expected - observed)^2 / expected
diffs
```

The sum of these values are called $\chi^2$ ("chi-squared").

```{r}
chi_squared <- sum(diffs)
chi_squared
```

If the value of chi-squared would be zero ($\chi^2=0$), we could conclude that there is no relation at all between sex and appreciation. If $\chi^2>0$, then it is clear that the size of $\chi^2$ will depend on the size of the table and the total number of observations. A contingency table with a lot of rows/columns will lead to a bigger $\chi^2$. 

**Cramér's V** $= \sqrt{\frac{\chi^2}{(k-1) \cdot n}}$ with $k$ either the number of columns or the number of rows, the smallest of both.

Cramér's V can be seen as a normalized version of $\chi^2$. The value is always a number between 0 and 1, and it is independent of the size of the contingency table.

```{r}
k <- min(nrow(observed), ncol(observed))
cramers_v <- sqrt(chi_squared / ((k - 1) * n))
cramers_v
```

Below you find the interpretation of the value of Cramér's V:

| Cramér's V | Conclusion             |
| :---:      | :---                   |
| 0          | No association at all  |
| 0.1        | Weak association|
| 0.25       | Fairly strong association |
| 0.50       | Strong association          |
| 0.75       | Very strong association     |
| 1          | Complete association       |

We can conclude that there is a fairly strong association between the variables `Geslacht` (sex) and `Keuze_basis` (appreciation of the base menu).

### R-functions

To calculate $\chi^2$ and Cramér's V in R, you can use the function `assocstats` from teh `vcd` library:

```{r}
library(vcd)
assocstats(observed)
```

You will recognize 2 values: "The Pearson's $\chi^2$" (line 3, value 1) and Cramér's V (last line).

## Regression

As an example for calculating the regression line, we take the dataset that is also used in the slides:

```{r}
weight_gain <- read.csv("../syllabus/data/santa.txt",
                        sep = '')
```

### Smallest squares method: detailed calculation

We try to approach a collection of points $(x_i, y_i)$ (for $i: 1, \ldots, n$) as best we can with a straight line $\hat{y} = \beta_0 + \beta_1 x$. The symbol $\hat{y}$ means "an estimate for $y$". The parameters $\beta_0$ and $\beta_1$ are calculated as follows:

$\beta_1 = \frac{\sum_{i=1}^{n}(x_i - \overline{x})(y_i - \overline{y})}{\sum_{i=1}^{n}(x_i - x)^2}$

$\beta_0 = \overline{y} - \beta_1 \overline{x}$

In R you can calculate this as follows:

```{r}
mx <- mean(weight_gain$x) # average of x
my <- mean(weight_gain$y) # average of y
xx <- weight_gain$x - mx # x - mx
yy <- weight_gain$y - my # y - my
beta_1 <- sum(xx * yy) / sum(xx^2)
beta_0 <- my - beta_1 * mx
```

A plot shows that this is a good approximation:

```{r}
plot(x = weight_gain$x, y = weight_gain$y,
     xlab = 'Protein content (%)',
     ylab = "Weight gain (g)")
abline(a = beta_0, # intersection y-axis
       b = beta_1, # directional coefficient
       col = 'red')
```

### Linear regression in R

Of course there is also a way to calculate this easily in R. You can use the function `lm()`, abbreviation for *linear model*.

```{r}
lm(weight_gain$y ~ weight_gain$x)
```

Note that the "group by" operator `~` is used. And that the independent variable ($x$) is located at the **right** of the tilde, while the dependent variable ($y$) is at the **left**. The result of `lm()` can immediately be given to the function `abline()`:

```{r}
plot(x = weight_gain$x, xlab = "Protein content (%)",
     y = weight_gain$y, ylab = "Weight gain (g)")
regression <- lm(weight_gain$y ~ weight_gain$x)
abline(regression,
       col = 'red')
```

## Covariance and Correlation

```{r}
families <- read.csv("../syllabus/data/families.txt", sep ="")
mx <- mean(families$x)
my <- mean(families$y)

plot (families $x, families$y)
abline(h = my, col = 'red')
abline (v = mx, col = 'red')
```

Covariance: $Cov(X,Y) = \frac{1}{n-1}\sum_{i=1}^n(x_i - \overline{x})(y_i - \overline{y})$

```{r}
# Covariance calculated manually
covar <- sum((families$x - mx) * (families$y - my)) / (length(families$x) - 1)
covar
# R function
cov (families$x, families$y)
```

Correlation (Pearson's correlation coefficient):

$R = \frac{Cov(X,Y)}{\sigma_x \sigma_y} = \frac{\sum(x_i - \overline{x})(y_i - \overline{y})}{\sqrt{\sum{(x_i-\overline{x})^2(y_i - \overline{y})^2}}}$

```{r}
# Calculating from covariance
covar / (sd(families$x) * sd(families$y))
# Calculating from the full formula
sum((families$x - mx) * (families$y - my)) / 
  sqrt(sum((families$x- mx)^2 * sum((families$y - my)^2)))
# R function
cor(families$x, families$y)
```

Coefficient of determination:

- Define $SS_{tot} = \sum_{i=1}^n(y_i - \overline{y})^2$, the total variance of the sample (SS is an abbreviation for *squared sum*).
- Define $SS_{res} = \sum_{i=1}^n(y_i - \widehat{y})^2$, the residuals with respect to the regression line, or the unexplained variance of the sample.
The coefficient of determination is then $R^2 = \frac{SS_{tot} - SS_{res}}{SS_{tot}}$

**Remark**: The formula in the slides is slightly different, but comes down to the same. In the slides we use:

- $\sigma_{y,total}^2 = \frac{SS_{tot}}{n} (= \sigma_y^2)$ and
- $\sigma_{y,unexpl}^2 = \frac{SS_{res}}{n}$.

A characteristic of the coefficient of determination is that it is the square of the correlation coefficient (which explains the notation $R^2$). This is a number between 0 and 1 that you can interpret as the percentage of the variance in $y$ that can be explained by the linaer model (and thus by the variance in $x$). This is a measure of how well the regression line approaches the real data points. The closer to 1, the better the approach is, so the closer the observed data points are to the regression line.

```{r}
# average of y
my <- mean(weight_gain$y)
# sum of squares w.r.t. the average
ss_tot <- sum((weight_gain$y - my)^2)
# sum of squares "residues", i.e. w.r.t. the predicted values (based on regression line)
regression <- lm(weight_gain$y ~ weight_gain$x) # regression model
yy <- predict.lm(regression, weight_gain)       # predicted values
ss_res <- sum((weight_gain$y - yy)^2)           # sum of squares
# R^2 according to the formula
(ss_tot - ss_res) / ss_tot

# R^2 by correlation:
correlation <- cor(x = weight_gain$x, y = weight_gain$y)
correlation^2
```

## Visualization of relationships between two variables

When you want to visualize the relationship between two variables, the most suitable chart type depends on the measurement level of the variables. These can be found in the table below for various combinations of measurement levels of the independent variable on the one hand and the dependent variable on the other hand.

| Independent  | Dependent    | Graphic type       |
| :---         | :---         | :---               |
| Qualitative  | Qualitative  | mosaic plot        |
|              |              | clustered bar plot |
|              |              | stacked bar plot   |
| Qualitative  | Quantitative | boxplot            |
|              |              | bar plot average   |
| Quantitative | Quantitative | scatter/XY plot    |

### Qualitative - Qualitative

#### Mosaic Plot

A mosaic plot is a graphical representation of a frequency table where the area of each tile is proportional to the frequency in the corresponding cell of the table.

```{r}
mosaicplot(t(observed),
           main = "Appreciation of the basic product range")
```

In this graph, the frequency table is transposed with the function `t()`. In this way the independent variable is shown in the columns and you can clearly see the differences in frequencies.

#### Clustered bar plot

```{r}
barplot(t(prop.table(observed, margin = 2)),
        beside = TRUE,
        legend = TRUE)
```

The values of the dependent variable ("Goed", ..., "Slecht") are plotted on the X axis.
There are clusters with a value (bar) for each independent variable (man/vrouw(woman)).
The bars show the *relative* frequencies (i.e. precentages) calculated relative to the total of the dependent variable.

The function `prop.table()` converts the counts into percentages. The `margin` parmeter  determines how the percentages are calculated: relative to the row totals (margin=1), relative to the column totals (margin=2) or relative to the grand total (no margin parameter).

```{r}
addmargins(observed)
addmargins(prop.table(observed, margin = 1))
addmargins(prop.table(observed, margin = 2))
addmargins(prop.table(observed))
```

#### Stacked bar plot

```{r}
proportions <- prop.table(observed, margin = 2)
barplot(proportions, horiz = TRUE)
```

This graph also shows *relative* frequencies. Notice the similarities with the mosaic diagram! However, a mosaic diagram provides some more information, because it is no longer clear in the bar diagram whether a different number of men or women has been interviewed.

### Qualitative - Quantitative

To represent the link between a qualitative independent and a quantitative dependent variable, we takethe question "Do staff spend a larger amount in the restaurant each week than students?" as an example."

#### Grouped boxplot

```{r}
boxplot(resto$Bedrag ~ resto$Klanttype)
```

#### Bar plot of averages

A chart type that we often encounter for this type of research questions is a bar plot of the average for each group:

```{r}
# first calculate the averages for each group
mean_amounts <- aggregate(resto$Bedrag ~ resto$Klanttype, FUN = mean)
# plot the bar chart
barplot(mean_amounts$`resto$Bedrag`,
        names.arg = mean_amounts$`resto$Klanttype`)
```

This type of graph provides **insufficient information** about the distribution of the averages.
This information can be added using *error bars*.
It is important to state in the caption of the chart what the error bars represent exactly.
They can show the standard deviation, but sometimes they show twice the standard deviation.

Unfortunately, there is no easy way to add error bars to the chart. 
The code below does the trick.

```{r}
# first calculate the averages and standard deviations for each group
mean_amounts <- aggregate(resto$Bedrag ~ resto$Klanttype, FUN = mean)
sd_amounts   <- aggregate(resto$Bedrag ~ resto$Klanttype, FUN = sd)
# get the maximum y value, to make the graph big enough
ymax <- max(resto$Bedrag, na.rm = TRUE)
        # "na.rm" is necessary to ignore a few NA values in the data set
        # without na.rm, max() would return "NA" instead of the maximum
# plot the bar graph
mean_plot <- barplot(mean_amounts$`resto$Bedrag`,
                     names.arg = mean_amounts$`resto$Klanttype`,
                     ylim = c(0,ymax))
        # mean_plot contains a vector with
        # the x values of (the middle of) the bars
# Draw the error bars
arrows(mean_plot,
       mean_amounts$`resto$Bedrag` - sd_amounts$`resto$Bedrag`,
       mean_plot,
       mean_amounts$`resto$Bedrag` + sd_amounts$`resto$Bedrag`,
       lwd = 1.5, angle = 90, code = 3, length = 0.15)
```

In most cases, a boxplot is a better chart that contains more information.
Especially if there are outliers in the data, the average and standard deviation will give a wrong picture of the spread.
As you can see for category C and G in the plot below.

```{r, echo=FALSE}
rb <- boxplot(decrease ~ treatment, data = OrchardSprays, col = "bisque")
title("Comparing boxplot and mean +/- SD")

mn.t <- tapply(OrchardSprays$decrease, OrchardSprays$treatment, mean)
sd.t <- tapply(OrchardSprays$decrease, OrchardSprays$treatment, sd)
xi <- 0.3 + seq(rb$n)
points(xi, mn.t, col = "red", pch = 18)
arrows(xi, mn.t - sd.t, xi, mn.t + sd.t,
       code = 3, col = "red", angle = 75, length = .1)
```

Also, average and standard deviation make no sense, if the dependent variable is not normally distributed.


### Quantitative - Quantitative

For this type of data typically an XY chart (scatter plot) is used. The independent variable is typically plotted on the X axis, the dependent one on the Y axis.

As an example, let's see if there is a connection between the volume of a car (variable `disp`, *displacement*, expressed in cubic inches) and its weight (variable `wt`, *weight*, in 1000 pounds) in the dataset `mtcars`:

```{r}
plot(mtcars$disp, mtcars$wt,
     xlab = "Volume (cu.in.)",
     ylab = "Weight (1000 lbs)")
regression <- lm(mtcars$wt ~ mtcars$disp) # calculate linear model
abline(regression, col = 'red')           # draw regression line in red
```

The `plot()` function will draw a little circle for each data point. You can of course change the symbol (cross, dot, asterisk, ...). See `?plot` for more information.

The regression line (in red) makes it clear that there is a positive relationship between volume and weight (as expected: larger cars are heavier).